0.1 Basic concepts sec 1.1 of the textbook
	variable: a characteristic that varies from a person or a thing to another one, or over time.
		notation: x, y, z
		example: height, # of m and m's in a bag
	experimental units (or unit): indiviudals or objects on which a variable is measured
		notation: i, j, k
	measurement: a single measurement is the value of a variable measured on a an experimental unit.
		notation: X_i (or y_i or z_i) denotes the measurement (or value) of variable X on unit i
	population: The set of all measurements of a variable of interest to the investigator
		note: we do not always observe all measurements in a population ie not all values in a population are known to us
	sample: A subset of measurements selected and observed from the population of interest
		note: Sample values are usually all observed and hene known to us
		observed sample values are also called "data" (or measurements)
		
0.2 Types of variables
	variables: 
			qualitative (categorical)
				pure qualitative (qualitative and non ordered) for example name
				qualitative and ranked (qualatative and ordered) for example GPA 
			quantitative
				discrete
				continuous
	
	key difference between qualitative and quantitative variables:
		numerical operators (+, -, x, /, primarily + and -) are meaningful for quantitative variables but not for qualitative variables
		examples: weight in pounds, quantitative (100lbs - 90lbs = 10lbs) meaningful
		group label: 1,2,3,4 qualatative
			1 + 3 does not mean group 4
		student #: qualatative/category +/- can be performed but not meaningful
	
	discrete vs continious quantitative
		discrete: possible values can be counted using integers (countable). In particular, a quantative numerical variable with fininity many possible values is disrete
		example: # of oranges on an orange tree in a grove
		continuous: Take infinitely many possible values which can fill an interval of decimal numbers
		note: an interval (a, b) where a < b is the set of all decimal numbers between a and b
		example: time until a light bulb burns out continious
			
	time spent on physicl exercises each week - continious quantitative
	number of text messages sent every day - discrete
	the level of pain from an injury 1,2,3...10 - ranked
	daily average temperature - continious
	number of textbooks bought in a term - discrete
	answer to: weather you on a car or not - pure qualitative
	types of makeups one owns - pure
	number of makeup product one owns - discrete
	monthly beer consumption by bottles - discrete
	
parameter: a numerical descripitive measure about popultion (usually unknown)
statistic: a numerical descripitive meaure calculated from the sample (or data)
note: you can obtain the value of a statistic using observed measurements in a sample, but you cannot obtain the value of a paramater using a sample
examples: some parameters: mu, sigma^2, sigma^2
		some statistics: sample mean x_bar sample variance, s_x^2, s_x
		
descriptive plots
	important: histogram
	other plotss: stem and leave plot, line char etc
	
bivariate data 
	a pair of variables x and y (written as (x, y))
	are observed simulteneously on n units i =1,2,...n resulting bivariate data: (x_i, y_i), i = 1,....,n
	
correlation coefficient (or simply correlation) between two variables
	r := s_xy / (s_x * s_y)
	
	Where s_xy (s is small letter) := (1 / n - 1) * sum i: 1 to n ( x_i - x_bar)(y_i - y_bar) is called the (sample) covariance between x an y. And x_bar = 1/n * sum i: 1 to n (x_i)
	(s_x)^2 = 1/n-1 * sum i: 1 to n (x_i - x_bar) ^ 2 = 1 / n - 1 * ( sum i: 1 to n(x_i^2) - 1/n * sum i: 1 to n(x_i)^2)
	
	s_x = sqrt(s_x^2). positive square root
	
	s_x^2 = sample variance
	s_x sample standard deviation
	
	the correlation coefficient r measures the strength of linear relationship between x and y
	
	r > 0 means positive linear, r < 0 means negative
	|r| closer to 1 means stronger cannot pass 1
	

continious distributions
important ones: normal distribuion, t-distribuion
normal distribution
	- shape of density curve = bell-shaped and symmetric
	- notation N( mu, sigma^2) = normal distribution with mean mu and variance sigma^2
	  If X follows N( mu, sigma^2) then we write X ~ N(mu, sigma^2)
	- If X ~ N(mu, sigma^2) then X + c ~ N(mu + c, sigma^2) for any constant c (ie a normal distribution with mean mu + c and variance of sigma^2)
	 in particular if X ~ N(0, sigma^2) then X + c ~ N(c, sigma^2)
	- If X ~ N(mu, sigm^2) then z := (X - mu) / sigma ~ N(0, 1) (standard normal distribuion)
	- 3 sigma rules: check textbook
	- how to use uppertail normal and t table
	
Hypothesis testing
	what is a hypothesis test. what are null hypothesis (H_o) and alternative (H_a)
	two tailed test and one tailed test
	type I and type II errors
	level of a test
	example: null hypothesis innocent until proven guilty, alternative guilty, type I error put innocent person into jail, two II didn't put bad guy away
	basic procedure to test a hypothesis
		1. State H_o and H_a
		2. Find the test statistic for the test
		3. Find the rejection or critial region or p-value
		4. Draw conclusion
	For a hypothesis test about a population mean, when to use z-test and when to use t-test

B_0: the mean value of y when X = 0
	reason:  the mean of y when x = 0 is: E(B_0 + B_1 * 0) = 0

B_1: The change in mean of y when x increases by one unit
	reason: mean of y for a given x: b_0 + b_1 * x mean of y for x+ 1 = b_0 + b_1 * (x+1)
	
assumng that x and y follos the SLR model and assuptions 1-5 hold, given the data (x_i, y_i) i = 1...n we may want to estimate
	estimate parameters B_0, B_1, sigma^2
		find out the linear relationship between x and y
	carry out hypothesis tests about B_1 or construct confidence intervals (CI) about it.
	given a future value, say x*, of the x variable. Prdict the corresponding value of y

finding the best fitting line and estiamte b_0 and b_1 using the method of least squares
	suppose that x and y follows a SLR model since b_0 and b_1 are unknown parameters we do not know the true line b_0 + b_1x fitting through the data.
	
	we aim to use the data to determine the best line fitting through the data by estimating b_0 and b_1 using the so called method of least squares

	a fitting line is defined by y_hat = b_0 + b_1 * x
	for a given pair of b_0 and b_1 values. different pairs of (b_0, b_1) valaues define different fitting lines
	For a given fitting line y_hat = b_0 + b_1 * x the squared fitting error for unit i given by (y_i - y_hat_i)^2 and the sum of squared errors for all units
	
	b_hat_1 = S_xy / S_xx and b_hat_0 = y_bar - b_hat_1 * x_bar
	where x_bar = 1/n * sum x_i. y_bar = 1/n * sum y_i
	
	S_xy = sum(x_i * y_i) - 1/n * sum(x_i) * sum(y_i) (as in assignment)
	and S_xx = sum((x_i - x_bar_)^2)
	
	b_hat_0 and b_hat_1 are called the least squares estimates of b_0 and b_1and the resutling fitting line y_hat = b_hat_0 + b_hat_1 * x also called the least squares line or regression line. y_hat_i is called the fitted value (or predicted value)
	
estiating sigma^2
	after obtaining the LS fitting line, the sum of squared fitting error is given by (SSE) is given by sum( (y_i - y_hat_i)^2) = sum(y_i - (b_hat_0 + b_hat_1 x_I))^2 under assumptions (1)-(5) of the SLR model an unbiased of sigma^2 (the variance of E_i) 
	
	s^2 = SSE / n - 2, n because summing n times in SSE and 2 because two estimated parameters. note n - 2 is called degrees of freedom (d.f) of SSE.
		n: # of terms to be summed
		2: # two estimated parameters
		
inference about slope parameter B_1
	Under the SLR model, if B_1 = 0 then y and  doesn't have a significant linear relationship and x will not be useful for explaining or predicting the value of y
	
	in this section, we study: if all assumptions (1) - (5) of the SLR model hold
		(i) how to use t-tet ot tet whether y and x have a significant linear relationship
		(ii) construct a confidence interval for B_1
		
review t-test for a normal mean when sigma is unknown
	the t-test for H_0: mu = mu_0 versus H_a: mu != mu_0 when the population standard deviation sigma is unknown
	test statistic t = (x_bar - mu_0)/(s_x/sqrt(n))
	t follows a t distribution with n-1 degrees of freedom (d.f)
		the d.f. of a t-test is the denominator in the expression s_2_x = (sum(x_i - x_bar)^2)/n-1
	for a alpha level test we reject H_0 in favor H_a if t > t_n-1;alpha/2 or t < t_n-1;alpha/2 and we do not reject H_0 otherwise
	
	a conceptual formula for the test statistic t
		x_bar estimator of mu
		s_x/sqrt(n) estimated standard error (s.e.) of the estimator (note the true s.e sigma /sqrt(n) is unknown)
		a conceptual formula for t = (estimator - mu) / (estimated s.e of the estimator)
		
	a (1 - alpha) confidence interval for mu is given by
		x_bar +- t_(n-1;alpha/2)(sx/sqrt(n))
		
		a conceptual formula for a (1 - alpha) t-interval of the normal mean
			estimator - t....(estimated s.e), estimator + t...(estimated s.e)
		
		
the mathematics achievement tests scores (x_i) and final calculus grades (y_i) for 10 randomly chosen college fresmen as given as follows
student math test score x_i final calculus grade y_i
1 39 65
2 43 78
3 21 52
4 65 82
5 57 92
6 47 89
7 28 73
8 75 98
9 34 56
10 52 75

find the least squares estimates of b_0 and b_1 as well as the least squares fitted regression line


finding a testing method for this problem
	the LS estimator B_hat_1 = S_xy/ S_xx is a statistic under Assumptins 1-5 of the SLR model, we can show that the sampling distribuion of B_hat_1 is given by 
	B_hat_1 ~ N(B_1, sigma^2/S_xx)
	
	ie a normal distribuion with mean B_1 (the parameter of interest) and s.e. sqrt(sigma^2, S_xx). Nte that the value of s.e. is unknown since sigma^2 is unknwown
	
	recall that sigma^2 can be estimated by s^2 = SSE / n - 2 so the s.e of B_hat_1 can be estimated by s.e_hat = sqrt( s^2/S_xx) = sqrt(SEE/n-e) / S_xx)
	
	noticing that B_1 is the mean of N(....) B_hat_1 is the estimator the mean B_1 with estimated s.e sqrt(s^2...) we can use a t-test to the test the hypothesis H_0: B_1  = 0 vs H_a: B_1 != 0
	
	test-statistic t = (B_hat_1 - 0) / sqrt(s^2/S_xx)
	
	under assumptions 1-5 and H_0: B_1 = 0 the above t statistic follows a t-distribuion with n - 2 d.f
	
	for a 2 level test if t> t_n-2;alpha/2 or t < - t_n-2;alpha/2 we reject H_0 in favor of H_a and conclude that there is a significant linear relationship between x and y;
	otherwise we do not reject H_0 and conclude that there is no significant linear relationship between x and y_bar

confidence interval for slope parameter B_1
	A (1 - alpha) level onfidnece interval for B_1 is given by
		(B_hat_1 +- t_n-2;alpha/2 * sqrt(s^2/S_xx))
		estimator - t...(estimated s.e)
		
ANOVA and F-test for testing significant linear relationship between x and y
	we now provide an alternative method for testing
	h_0: B_1 = 0 vs H_a: B_1 != 0
	(the same hypothesis testing problem as that for t-test)
	using the so called (an)alysis (o)f (va)riance or ANOVA
	
	ANOVA and t-test for single regression both give the same conclusion
	
ANOVA
	total sum of squares (TSS)
		TSS := sum(y_i - y_bar)^2 = S_yy
		associated d.f: n-1
	sum of square for regression SSR:
		SSR: sum(y_hat_i - y_bar)^2 = sum(b_hat_0 - b_hat_1 * x_i - y_bar)^2 = (S_xy)^2/S_xx
		associated d.f: 1
	mean square of regression (MSR)
		MSR = SSR / (d.f of SSR) = SSR / 1
	sum of squares of error (SSE):
		SSE = S_yy - (S_xy)^2 / S_xx
		associated d.f: n - 2
	mean square of error MSE
		MSE = SSE / df of SSE = SSE / n - 2 = s^2
		
	by the above formulas we can see that TSS = SSR + SSE and d.f of TSS = d.f of SSR + d.f of SSE 
	
source d.f SS MS F value
regression 1 (150^2 / 196) (150^2 / 196)
error 14 233 - (150^2 / 196) (233 - 150^2/196) / 14
total  15 233

F test for th slope paramater B_1
	F = MSR / MSE = (SSR / 1) / (SSE / n-2)
	
	undr the null hypothesis h_0: b_1 = 0 the test statistic F follows a so called F distribution ith the numerator d.f being 1 and the denominator d.f being n-2 this F distribution is denoted as F_1,n-2 
	only takes positive values, non-symmetric
	
	rejection region
		for a 2 level test, when F value (value of the F test statistics) satisfies F > n_1,n-2;alpha we reject H_0 in favor of H_a and conclude that there i a significant linear relationship between x and y. If F <= F_1,n-2;alpha we do not reject H_0 and conclude there is no significant linear relationship between x and y.
		
why do we reject H_): B_1 = 0 when F-value is large
	conceptual reason
		recall F = SSR / SSE
		SSR = sum(predicted y - y_bar)^2
		SSE = sum(y_i 0 y_hat_i)^2
		
		If B_1 != 0 ie x is useful for explaining y_i then y_i - y_hat_i should be small on average as we learned, which gives a small value of SSE
		at the same time y_hat_i should be close to y_i which makes SSR close to S_yy.
		
		On the other if B_1 = 0 then x is not useful for explanng y. Hence the squared fitting error (y_hat_i - y_i)^2 should be large, on average. In addition B_hat_1 approximately equal to B_1 = 0 => B_hat_0 = y_bar - B_hat_1 * x_bar close to y_bar
		and hence SSR close to 0
		
		When B_1 != 0 F = SSR/SSE where SRR is close to S_yy and SSE is small ie F is large
		When B_1 = 0 F = SSR / SSE where SSR close to 0 and SSE = large (close to S_yy) therefore a small F -value
		
Remarks about F tet and t test under SLR
	Under SLR model suppose that asumptions 1-5 hold, 
		both t test and F test are for testing the same hypothesis
			H_0: B_1 = 0 vs H_a: B_1 != 0
		t test statistic value and F test statistc value are not the same however the conclusion of the t test and of that F test are always the same ie both reject H_0 or neither rejects H_0
		
Coefficient of determinaton r^2
	TSS = sum(y_i - y_bar)^2 a measure of total variation
	SSR = sum(y_hat_i - y_bar)^2 the portion of the total varation that is expained by the regression of y on x
	SSE = TSS - SSR the portion of TSS that is unexplained by the regression of y on x due to random error
	
The coeffficient of determination denoted r^2 is defined as r^2 = SSR / TSS
	r^2 measures the percentage of the total variation in y that is explained by the regression of y on x
	a large value of r^2 suggests that the regression of y on x is a useful for explaining the total variation in y. A small value of r^2 suggests that the regression of y on x is not very useful
	
	r^2 = (S_xy / sqrtS_xx * sqrtS_yy)^2
	
	Hence r^2 also measures the strength of the linaer relationship between x and y
	larger r^2: stronger linear relationship between x and y and more TSS explainded
	
	since correlation coeffficient r satisfies -1 <= r <= 1 we have 0 <= r^2 <= 1
	
	The r^2 value can be calculated using the ANOVA table: r^2 = SSR / TSS

Confidence interval for the mean line E(y) = B_0 + B_1 * x
	Recall Y ~ N(B_0 + B_1 * x, sigma^2)
	In SLR we assumed that this relationship is true for any observed (y_i, x_i) and aso true for any future or unobserved pair.
	This imlies that E(y) = B_0 + B_1 * x and hence the straight line B_0 + B_1 * x is called the mean line for y
	
	now for a new value say x* of x denoate the corresponding y value under the SLR as y_x* ie y_x* = B_0 + B_1 * X* + epsilon
	
	we have E(y_x*) = B_0 + B_1 * X*, E(y_x*) a paramter
	
	given x* we can obtain the estiamted value of y_x* (the true value is unknown) by y_hat_x* = b_hat_0 + b_hat_1 * x* (a statitic)
	
	it can be shown that, under assumptions 1-5, the sampling distribution of y_hat_x* is
	
	
	